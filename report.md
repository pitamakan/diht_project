# Отчет #

## Введение ##
(Мини-пакетный) стохастический градиентный спуск является популярным методом оптимизации, который был применен ко многим приложениям машинного обучения. Но довольно
высокая дисперсия, вносимая стохастическим градиентом на каждом шаге, может замедлить сходимость. В этой статье мы предлагаем антитетическую стратегию выборки для уменьшения дисперсии, используя преимущества внутренней структуры в наборе данных. В соответствии с этой новой стратегией стохастические градиенты в мини-партии больше не являются независимыми, но максимально отрицательно коррелируют, в то время как стохастический градиент мини-партии по-прежнему является беспристрастной оценкой полной gradient. Для задач бинарной классификации нам  просто нужно вычислить антитетические образцы заранее и повторно использовать результат в каждой итерации, что является практичным. Приведены эксперименты, подтверждающие эффективность предложенного метода.

## Алгоритм ##

## О полигоне, тестах и сравниваемых алгоритмах ##
См. .ipynb 
## Результаты ##

## Вывод ##
